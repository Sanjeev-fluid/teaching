{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAP 631 Lab 4: Classification of the CIFAR-10 dataset using CNNs\n",
    "\n",
    "J.B. Scoggins\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jbscoggi/teaching/blob/master/Polytechnique/MAP631/cnn.ipynb) \n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jbscoggi/teaching/master?filepath=Polytechnique%2FMAP631%2Fcnn.ipynb)\n",
    "\n",
    "The [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) provides 60000 32x32-pixel images, classified into 10 categories.  The figure below provides a random sample of some images in each category.\n",
    "\n",
    "![](artwork/cifar_images.png)\n",
    "\n",
    "During this session, you will learn how to build a Convolutional Neural Network (CNN), which (when trained) will be able to automatically classify new images into one of these categories.  We will make use of the [Keras library](https://www.tensorflow.org/guide/keras) which provides a high-level interface to TensorFlow, which you have already seen.\n",
    "\n",
    "## Introduction to Keras\n",
    "\n",
    "Keras is a high-level API to build and train deep learning models. It's used for fast prototyping, advanced research, and production, with three key advantages:\n",
    "\n",
    "- __User friendly__: Keras has a simple, consistent interface optimized for common use cases. It provides clear and actionable feedback for user errors.\n",
    "- __Modular and composable__: Keras models are made by connecting configurable building blocks together, with few restrictions.\n",
    "- __Easy to extend__: Write custom building blocks to express new ideas for research. Create new layers, loss functions, and develop state-of-the-art models.\n",
    "\n",
    "In Keras, models are built by assymbling multiple layers.  Suppose we want to create a new multilayer perceptron model to categorize 128-feature data into 10 labeled categories.  The Keras code could look like:\n",
    "\n",
    "```python\n",
    "# Create a sequential model\n",
    "model = tf.keras.Sequential()\n",
    "# Adds a densely-connected layer with 64 units to the model\n",
    "model.add(layers.Dense(64, activation='relu'), input_shape=[128])\n",
    "# Add another\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "# Add a softmax layer with 10 output units\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "```\n",
    "\n",
    "The `input_shape` argument must be given for the first layer in the model, however all other layers will automatically determine the input shape based on the previous layer in the model.  Note that the code above is substantially simpler than the corresponding TensorFlow code.  This is particular useful for building convolutional or other types of layers, as we will see.\n",
    "\n",
    "Once built, a model's learning can be configured with the `compile()` function\n",
    "\n",
    "```python\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer=tf.train.AdamOptimizer(0.001), \n",
    "    metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "In this case, a cross-entropy loss function is used with the ADAM optimization algorithm.  The `metrics` argument allows the model to keep track of a number of [training metrics](https://www.tensorflow.org/api_docs/python/tf/keras/metrics) during training.\n",
    "\n",
    "Once configured, training is performed using the `fit()` function.\n",
    "\n",
    "```python\n",
    "model.fit(data, labels, epochs=10, batch_size=32)\n",
    "```\n",
    "\n",
    "The function takes an array-like (could be numpy array) of data and the corresponding target values, and performs the optimization of the learnable parameters in the model.  See the documentation for the [fit()](https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#fit) function for more details.\n",
    "\n",
    "Once trained, the model can be used to predict, using the `predict()` function. \n",
    "\n",
    "```python\n",
    "prediction = model.predict(new_data)\n",
    "```\n",
    "  \n",
    "## Tasks\n",
    "Begin by importing the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense, Flatten, Activation\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand the dataset\n",
    "\n",
    "Understanding your dataset is the first prerequisit to training any model.  The CIFAR-10 dataset is provided directly from Keras.  \n",
    "- Download the dataset. See [`tf.keras.datasets`](https://keras.io/datasets/) for how to download the data, and what format it is provided.  Note that the dataset is already divided into a training set of 50000 images, and a test set of 10000.\n",
    "- Verify that the shape of the image and target arrays are what you expect.\n",
    "- Create a list of labels corresponding to the 10 categories.  This will be used to convert the 0-9 digits in the target arrays to string labels. The categories are labeled as follows\n",
    "  0. airplane\n",
    "  1. automobile\n",
    "  2. bird\n",
    "  3. cat\n",
    "  4. deer\n",
    "  5. dog\n",
    "  6. frog\n",
    "  7. horse\n",
    "  8. ship\n",
    "  9. truck\n",
    "- Visualize some images in each category using the `imshow()` function in `matplotlib.pyplot`.  Can you recreate the figure at the top?  Hint: the top figure was created using the first 8 images belonging to each category in the training data.\n",
    "- Normalize the image data from [0,255] to be [0,1].  Normalizing improves model training.  (to test this, you can comment out the normalization later)\n",
    "- Lastly, convert the target arrays to one-hot encodings.  Hint: checkout the [`tf.keras.utils.to_categorical()`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar_data():\n",
    "    \"\"\"\n",
    "    Loads the CIFAR-10 dataset using Keras and preprocess for training.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return x_train, y_train, x_test, y_test, labels\n",
    "\n",
    "\n",
    "\n",
    "x_train, y_train, x_test, y_test, labels = load_cifar_data()\n",
    "\n",
    "# Visualize the dataset\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First naive model\n",
    "\n",
    "In order to better understand the importance of CNNs, it is instructive to first see how well a naive dense network performs on the dataset.\n",
    "\n",
    "- Create a sequential model with 4 `Dense` hidden layers of 2048, 1024, 512, and 256 nodes each, with ReLU activation, and a linear output layer of 10 nodes.  Note that you will need to use the `Flatten` layer first in order to convert the 3D (x, y, rgb) image data into 1D.\n",
    "- Compute by hand the total number of trainable parameters (weights and biases) in the model.\n",
    "- Compile the model with a `categorical_crossentropy` loss, using the SGD optimizer, including the `accuracy` metric.\n",
    "- Use the `summary()` function on model to get a text summary of the model.  Did you compute the number of parameters correctly?\n",
    "- Train the model:\n",
    "  - Start with small batch size of 32 and train for 10 epochs\n",
    "  - Use early stopping on the validation accuracy with a patience of 2\n",
    "- How does the model perform? Is it any better than a random guess? \n",
    "- Try changing the batch size to see if there is any improvement.\n",
    "- Try adding batch normalization after each hidden layer.  Any better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_model(input_shape, num_classes):\n",
    "    model = # TODO\n",
    "    model.compile( #TODO )\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, x, y, x_test, y_test, batch_size=128, epochs=10):\n",
    "    # TODO\n",
    "\n",
    "model = dense_model(x_train.shape[1:], 10)\n",
    "train_model(model, x_train, y_train, x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Net\n",
    "\n",
    "Convolutional neural networks allow us to do drastically better on this dataset (and many image classification problems in general).  In this task, you will build your first convolutional network and see how it performs during training.\n",
    "\n",
    "- Create a new model with the following layers\n",
    "  - 3x3 2D convolution with zero padding (same), 32 filters\n",
    "  - ReLU activation\n",
    "  - 3,3 2D convolution, no padding, 32 filters\n",
    "  - ReLU activation\n",
    "  - Max pooling with size (2,2)\n",
    "  - 3x3 2D convolution, no padding, 64 filters\n",
    "  - ReLU activation\n",
    "  - 3x3 2D convolution, no padding, 64 filters\n",
    "  - ReLU activation\n",
    "  - Max pooling with size (2,2)\n",
    "  - Flatten\n",
    "  - Dense layer with 512 nodes, ReLU activation\n",
    "  - Softmax output layer with 10 nodes\n",
    "- Compile the network with same optimizer and metrics as the dense network.\n",
    "- Compute by hand the number of trainable parameters in this network.  Are there more or less than the more simple dense network?  Why?  Confirm with `summary()`.\n",
    "- Use the same training procedure as before for 10 epochs and batch size of 32.\n",
    "- How does the validation accuracy change with each epoch?\n",
    "- Increase the batch size to 64 and retrain.  Better or worse?  Try 128 as well.  How does increasin the batch size improve the training?\n",
    "- Note how the validation accuracy begins to decrease at some point, while the training accuracy continues to increase.  What is this phenomena called?  Try adding 3 dropout layers to the model, one before each max pooling layer and one before the last layer, using a dropout ratio of 0.25.  Does this improve over-fitting?\n",
    "- Play with batch normalization.  For example, add batch normalization layers after each dropout layer.  Do you notice a faster increase in the model improvement? Why?\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(input_shape, num_classes):\n",
    "    # TODO\n",
    "    return model\n",
    "\n",
    "# Create and train model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make some predictions\n",
    "\n",
    "Assuming all went well during the previous tasks, you can now predict the category of a new image!  Here are a few examples of my predictions:\n",
    "\n",
    "![](artwork/cnn_predictions.png)\n",
    "\n",
    "- Use `predict` on your trained model to test its prediction on a few example images.  You can use images taken from the test dataset, as these were not used to train your model.  Hint: at this point, it is probably convenient to use the `save` and `load_model` functions from Keras.  You can save the model after training it, and then decide to load from saved file instead of building a new one (if available) on successive runs.\n",
    "- Using `imshow` and `hbar` from `matplotlib.pyplot`, try to recreate the image above for a few example images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a _confusion matrix_. A confusion matrix is often used in supervised learning to understand how well (or not) each category is being classified.  Each element (i,j) in the confusion matrix represents the predicted class j for each true class i.  Consider the following 10 predictions for a 2 category model predicting male or female.\n",
    "\n",
    "| example     | true category  | predicted category  |\n",
    "|-------------|----------------|---------------------|\n",
    "| 1           | male           | male                |\n",
    "| 2           | female         | male                |\n",
    "| 3           | female         | female              |\n",
    "| 4           | male           | male                |\n",
    "| 5           | male           | female              |\n",
    "| 6           | male           | male                |\n",
    "| 7           | female         | female              |\n",
    "| 8           | male           | female              |\n",
    "| 9           | female         | female              |\n",
    "| 10          | female         | female              |\n",
    "\n",
    "Based on the above data, the model is accurate 70% of the time.  The confusion matrix is\n",
    "\n",
    "|        | male | female |\n",
    "|--------|------|--------|\n",
    "| male   | 3    | 2      |\n",
    "| female | 1    | 4      |\n",
    "\n",
    "The confusion matrix gives us more information than a simple accuracy measurement.  In this case, we see that the class female has a higher accuracy over male.  Create the confusion matrix the CIFAR-10 dataset using the test data.  What does it tell you about the relationships between each class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further exercises \n",
    "\n",
    "- Try training on additional datasets available with Keras.  For example, the MNIST data (hand-written digits) will require virtually no code changes to solve.\n",
    "- Play with different CNN architectures.  Can you beat the performance of the net used in this lab?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
