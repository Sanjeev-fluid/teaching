{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAP 631 Lab 5: Text Generation using Recurrent Neural Networks\n",
    "## Solution\n",
    "\n",
    "J.B. Scoggins\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jbscoggi/teaching/blob/master/Polytechnique/MAP631/rnn.ipynb) \n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jbscoggi/teaching/master?filepath=Polytechnique%2FMAP631%2Frnn.ipynb)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Recurrent neural networks (RNNs) have emerged as powerful predictive and generative models for a range of applications.  For example, take a look at the excellent blog post by Andrew Karpathy on [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/).  In this lab assignment, your task is to build a generative, character-by-character RNN that can predict the next character from a given sequence. The classic work *The Odyssey* by Homer will serve as a corpus for your model.  An example of the type of text that you can generate from this lab is below.  The `seed` text represents an initial sequence of characters that is randomly sampled from the corpus.  Following the seed, you can see that the model has predicted a fairly realistic text sequence, in the style of the *The Odyssey*, including realistic line breaks and punctuation.\n",
    "\n",
    "<b>Seed text</b>\n",
    "```\n",
    "h ulysses for having\n",
    "blinded an eye of p\n",
    "```\n",
    "\n",
    "<b>Prediction of next 500 characters</b>\n",
    "```\n",
    "olypels end she darte yod mentered saw he would polden ewall by sur; for her got him eather, and he would send\n",
    "them flying out of the hould not save his\n",
    "men, for they perished through their own sheer folly in eating the\n",
    "cattle of the sun-god hyperion; so the god prevented them from ever\n",
    "reaching home. tell me, too, about all these things, oh daughter of\n",
    "jove, from whatsoever source you may know them.\n",
    "\n",
    "so now all who escaped death in battle or by shipwreck had got safely\n",
    "home except ulysses, and \n",
    "```\n",
    "\n",
    "### RNN Models in Keras\n",
    "\n",
    "As in the CNN lab, we will use the Tensorflow Keras module to implement a simple RNN model.  Specifically, we will make use of the [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM) and [GRU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU) layers.\n",
    "Before proceeding, it may be useful to familiarize yourself with the [Keras API for RNNs](https://www.tensorflow.org/guide/keras/rnn).  In particular, understanding what input shape each RNN layer expects will be crucial.\n",
    "\n",
    "### Embeddings\n",
    "\n",
    "Embeddings is a topic you will see later in the course, but we will utilize them in this lab to make the model development easier.  In essence, an embedding encodes an integer index as a vector of some size.  You may think of this as a generalization of a one-hot encodding.  For example, consider the list of characters in the word \"hello\".  If this word contained all the characters in our vocabulary (namely \"h\", \"e\", \"l\", and \"o\"), we can generate a one-hot encoding where each character is represented by a vector of size 4, with a 1 in the element corresponding to the letter, and zeros everwhere else.  For our 4-character vocabulary, this could look like the following:\n",
    "\n",
    "| char / index | encodding  |\n",
    "|--------------|------------|\n",
    "|h / 0         | 1 0 0 0    |\n",
    "|e / 1         | 0 1 0 0    |\n",
    "|l / 2         | 0 0 1 0    |\n",
    "|o / 3         | 0 0 0 1    |\n",
    "\n",
    "While one-hot encoddings are usefull for many applications, they have a number of limitations.  For example,\n",
    "\n",
    "- The encodding matrix is extremely sparse\n",
    "- The size of the encodding depends on the size of the vocabulary or number of categories being encodded\n",
    "- There is no notion of similarity between the entities being encodded\n",
    "\n",
    "An embedding solves these problems by using a learnable (size of vocabulary)X(size of encodding) matrix, in place of the fixed and sparse one-hot encodding matrix.  Continuing with our previous example, an embedding for the characters in \"hello\" might take the form\n",
    "\n",
    "| char / index | encodding   |\n",
    "|--------------|-------------|\n",
    "|h / 0         | 1.2 0.3 4.3 |\n",
    "|e / 1         | 0.1 1.5 7.8 |\n",
    "|l / 2         | 0.5 3.2 1.9 |\n",
    "|o / 3         | 3.6 7.2 5.8 |\n",
    "\n",
    "Note that here, we have chosen an encoding represented by a vector of size 3, which is less than the size of the vocabulary.  This is called \"embedding\" our vocabulary in a 3 dimensional space.  This is extremely useful when building encoddings for very large vocabularies.  In addition, the coefficients in the encodding matrix are learned during the training process, allowing \"similar\" characters (in this case) to be grouped locally in the encodding space.  Further, we can visualize the embedding space through a number of techniques to help us understand how our vocabulary is encodded.  We will not do that here, but you can find a number of examples online, if you are interested.\n",
    "\n",
    "### Grading\n",
    "\n",
    "This lab will be **optionally** graded for those who wish.  If you want this lab graded, please submit the following files to Moodle **before midnight on Oct. 13**:\n",
    "\n",
    "- completed jupyter notebook\n",
    "- saved model parameters file, needed to run your model (I will not retrain your model)\n",
    "\n",
    "*Submissions after the due date will not be graded.*  Note that your code must successfully load the model parameters and run one epoch of training, showing the accuracy during that epoch.  Your grade will be based on the completeness of the tasks (your code) and the reported accuracy of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "Import packages and intialize various global variables. You may want to change these later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Global constants\n",
    "window_size = 40 # Length of character sequences\n",
    "batch_size = 32  # Batch size for learning\n",
    "rnn_units = 128 # Number of hidden units in the LSTM or GRU cells cell\n",
    "epochs     = 100 # Number of training epochs\n",
    "\n",
    "text_file = 'data/odyssey.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Preprocess text data\n",
    "\n",
    "Fill in the function below to read in the text file given by `text_file` and perform any preprocessing that you feel is necessary.  For example, convert the text to lower case in order to reduce the size of the vocabulary.  Other examples of processing include replacing accented characters with non accented characters, removing \"unnecessary\" punctuation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_file(file_name):\n",
    "    \"\"\"Read a text file, perform preprocessing, and return text as a string.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        Name of text file to load.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    text : str\n",
    "        Preprocessed text from the file.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Read in file and convert to lower case\n",
    "    text = \n",
    "\n",
    "    # Optional: perform additional processing\n",
    "\n",
    "    return text\n",
    "\n",
    "# Load and prepare data\n",
    "text = preprocess_file(text_file)\n",
    "text = text[:10000] # Shorten text for testing\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Generate a dataset for training\n",
    "\n",
    "Fill in the function below which takes in the document text and a \"window\" size and returns a list of unique characters representing the vocabulary for the document as well as the training data.  The training data consists of two lists.  The first is a list of lists of integers (indexing the vocabulary list) corresponding to a sequences of characters found in the document of length `window_size`.  The other is a list of integers (indexing the vocabulary list) corresponding to the next character in the sequence, for each sequence in the first list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(text, window_size=40):\n",
    "    \"\"\"Create the dataset used to train the RNN.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        String representing text to learn on.\n",
    "    window_size : int\n",
    "        Length of character sequence used to predict next character.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    vocab : list(char)\n",
    "        List of characters making up the vocabulary of the text.\n",
    "    x_data : list(list(int))\n",
    "        List of sequences of size window_size, containing indices into vocab.\n",
    "        Each sequence represents a sequence of window_size characters found in\n",
    "        the text.  The number of sequences generated will be len(text) - window_size.\n",
    "    y_data : list(int)\n",
    "        List of indices corresponding to the characters that follow the\n",
    "        sequences in x_data.\n",
    "    \"\"\"\n",
    "    # TODO: Determine list of unique characters\n",
    "    vocab = \n",
    "    \n",
    "    x_data = []\n",
    "    y_data = []\n",
    "\n",
    "    # TODO: Generate training data\n",
    "    \n",
    "    \n",
    "    return x_data, y_data, vocab\n",
    "\n",
    "x_data, y_data, vocab = make_dataset(text, window_size=window_size)\n",
    "\n",
    "# Check if everything is working\n",
    "print(vocab)\n",
    "print(len(vocab))\n",
    "print(x_data[0])\n",
    "print(y_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Create the RNN model\n",
    "\n",
    "Fill in the function below which builds and returns the RNN model, for the given size parameters and RNN layer.  The model should take as input a tensor representing batches of character index sequences and output a tensor representing the probabilities of each character in the vocabulary coming next in the sequence, for each sequence in the batch.  Use the following architecture.  \n",
    "\n",
    "- Sequential Keras model\n",
    "  - [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) with input and output dimensions equal to the vocab size (you can try using smaller encoddings later)\n",
    "  - [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM) with num_units\n",
    "  - [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) output, with softmax\n",
    "- `sparse_categorical_crossentropy` loss\n",
    "- Adam optimizer\n",
    "- Metrics: accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_model(num_units, window_size, vocab_size, rnn_layer=layers.LSTM):\n",
    "    \"\"\"Creates the RNN model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_units : int\n",
    "        Number of hidden units in the LSTM layer.\n",
    "    window_size : int\n",
    "        Number of characters in an input sequence.\n",
    "    vocab_size : int\n",
    "        Number of unique characters in the vocabulary.\n",
    "    rnn_layer : Keras RNN layer (RNN, LSTM, GRU)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : Keras model\n",
    "        RNN model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Build the model\n",
    "    model = \n",
    "    \n",
    "    # TODO: Compile the model\n",
    "\n",
    "    return model\n",
    "\n",
    "model = rnn_model(rnn_units, window_size, len(vocab))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Train and evaluate\n",
    "\n",
    "Fill in the code below to train the RNN model.  After every 3 epochs, generate 500 characters of text from a random seed sequence to gauge how well the model is doing.  This can be done by using the seed to predict the next character in the sequence (take the maximum likelihood character).  Append this new character onto the sequence (dropping the first character to maintain the window size) and repeat.  Print each new character as you go to generate the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "for i in range(1,epochs):\n",
    "    # TODO: Fit model for 3 epochs\n",
    "\n",
    "    # TODO: Generate text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Repeat your experiment using a GRU layer\n",
    "\n",
    "Repeat tasks 3 and 4 with the GRU layer in place of the LSTM.  Do you notice any differences in the performance, training, or text generation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you have time...\n",
    "\n",
    "This lab was a small taste of the power of RNN models.  Here are some other things you can try if you want to go further with the time you have left.\n",
    "\n",
    "- Sample the output distribution from the model to generate the next character in the sequence (instead of taking the most probable).  This will add some more randomness to your text generation.\n",
    "- Build a vocabulary of words, rather than characters.  This will highlight the importance of the embedding layer (you will need to use a smaller output dimension for the embedding than the vocabulary size).\n",
    "- Try other media types (eg: sound, video, guitar tabs...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
