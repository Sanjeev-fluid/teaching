{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 204 Lab 13: Reinforcement Learning II\n",
    "\n",
    "Jesse Read, J.B. Scoggins\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jbscoggi/teaching/blob/master/Polytechnique/CSE204/Lab13_answers.ipynb) \n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jbscoggi/teaching/master?filepath=Polytechnique%2FCSE204%2FLab13_answers.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This is the second lab dealing with reinforcement learning.  Today, the goal will be to implement and train an agent to balance a pole on top of a moveable cart using the hill climbing algorithm discussed in the lecture.  We will again use the [OpenAI Gym](https://gym.openai.com) library that you saw in the last lab for creating the environment for our agent to act in.  To get started, import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
    "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
    "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import choice as sample\n",
    "from numpy.random import rand, randn\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, just run the cell below which provides utility functions for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Utility functions to enable video recording of gym environment and displaying it\n",
    "To enable video, just do \"env = wrap_env(env)\"\n",
    "https://star-ai.github.io/Rendering-OpenAi-Gym-in-Colaboratory/\n",
    "\"\"\"\n",
    "\n",
    "from gym import logger as gymlogger\n",
    "gymlogger.set_level(40) #error only\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "def show_video():\n",
    "  mp4list = glob.glob('video/*.mp4')\n",
    "  if len(mp4list) > 0:\n",
    "    mp4 = mp4list[0]\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "  else: \n",
    "    print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "  env = Monitor(env, './video', force=True)\n",
    "  return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only in colab!\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding with the excercise, you can run the following cell to see the cart-pole problem in action with an agent that takes random actions at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = wrap_env(gym.make('CartPole-v1'))\n",
    "env.reset()\n",
    "for _ in range(200):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        env.reset()\n",
    "env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unless you got very lucky, you will notice that the pole on top of the cart fell and possibly spun around the cart several times.  In fact, this problem is physically unstable, and without a proper controller, the pole will always fall.  In order to stop the pole from falling, the agent is given two possible actions: 1) apply a positive force +1 or 2) apply a negative force -1.  The response to this force depends on the friction of the cart on the track, and the pole on the cart, the mass of both the cart and the pole, and the horizontal and angular velocities and the cart and pole respectively.  At any given point in time, the state of the system is fully characterized by 4 variables:\n",
    "\n",
    "1. The horizontal location of the cart\n",
    "2. The horizontal velocity of the cart\n",
    "3. The angular position of the pole\n",
    "4. The angular velocity of the pole\n",
    "\n",
    "Given those four quantities and the force imposed on the cart, the system behavior is governed by a set of differential equations, which are hidden from you and the agent.  Your task therefore, is to build an agent which learns from the response of the cart-pole system to various force inputs, such that it can create a policy for balancing the pole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hill Climbing\n",
    "\n",
    "As you may recall, hill climbing is a simple but powerful learning technique.  Here is a quick refresher if you don't remember the details.  First, let's recall a bit of the terminology.\n",
    "\n",
    "1. We have an environment (in this case the cart-pole problem).\n",
    "2. An agent maps observations of the state of the environment $\\mathcal{S}$ to allowed actions $\\mathcal{A}$ through a policy $\\pi : \\mathcal{S} \\mapsto \\mathcal{A}$.\n",
    "3. A reward function provides a metric to measure the benifit of taking an action for a given state.\n",
    "4. Ideally, the best policy is one that maximizes the return, which is the sum of the rewards for a series of actions over a finite amount of time.\n",
    "5. A value function maps a given state to the expected return given the best policy.\n",
    "\n",
    "In this lab, we will work with policy learning, which means that we will search for the best policy that maximizes the return value.  Specifically, we consider policies of the form\n",
    "\n",
    "$$ a_t = \\pi_\\theta(s_t) = \\text{argmax} \\;\\sigma(W^T s_t + b) $$\n",
    "\n",
    "where $a_t$ is the action at time $t$, given the policy and state at time $t$.  The policy parameters $\\theta = \\{W, b\\}$ represent the weights and biases for the linear transformation of the state space into action space, and $\\sigma$ is a nonlinear operator, such as the sigmoid function.  Note this is just a single layer perceptron, like the ones you have dealt with in previous labs on neural networks.  The difference here, however, is that we will not train this network with gradient descent, but rather with the hill climbing algorithm, which is summarized as follows.\n",
    "\n",
    "---\n",
    "**Algorithm 1: Random Optimization in Continuous Search Space (Hill Climbing)**\n",
    "1. Initialize weights and biases, $\\theta$\n",
    "2. Select minimal return desired, $g_{\\text{min}}$.\n",
    "3. Loop while $g(\\theta) < g_{\\text{min}}$\n",
    "   1. Sample nearby parameters, $\\theta' \\sim N(\\theta, \\text{variance})$\n",
    "   2. if $g(\\theta') > g(\\theta)$:\n",
    "      1. Update parameters, $\\theta \\gets \\theta'$\n",
    "---\n",
    "\n",
    "In the algorithm above, $g(\\theta)$ is the return, i.e. the sum of rewards for a series of actions taken with the same policy ($\\theta$).  Note also that the perturbation variance is a hyperparameter in our algorithm.  Finally, we can modify the algorithm above by taking random resets during sampling.  In other words, take a random sample centered at zero, rather than at the current parameter vector, every so often.  This helps to eliminate stalling and poor convergence. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Implement policy function\n",
    "\n",
    "**Exercise 1.1**: Fill in the code below to implement the SLP policy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(a):\n",
    "    '''Linear activation.'''\n",
    "    return a\n",
    "\n",
    "def sigmoid(a):\n",
    "    '''Sigmoid activation.'''\n",
    "    return 1.0 / (1.0 + np.exp(-a))\n",
    "\n",
    "class SLP():\n",
    "    '''A Single-Layer Perceptron with copy and random modification.'''\n",
    "\n",
    "    def __init__(self, inputs, outputs, activation = linear):\n",
    "        self.input_size = inputs\n",
    "        self.output_size = outputs\n",
    "        self.reset()\n",
    "        self.activation = activation\n",
    "\n",
    "    def reset(self, scaling=0.0):\n",
    "        '''Resets the weights and biases to normal distribution with scaling.'''\n",
    "        # TODO\n",
    "        self.W = \n",
    "        self.b = \n",
    "\n",
    "    def predict(self,x):\n",
    "        '''Feedforward prediction of SLP.'''\n",
    "        # TODO\n",
    "        return ?\n",
    "\n",
    "    def copy(self, modify=False):\n",
    "        '''Creates a copy of this SLP, with possible modification.'''\n",
    "        \n",
    "        # Create a copy\n",
    "        b = SLP(self.input_size, self.output_size)\n",
    "        b.W = self.W.copy()\n",
    "        b.b = self.b.copy()\n",
    "\n",
    "        if modify:\n",
    "            b.modify()\n",
    "\n",
    "        return b\n",
    "\n",
    "    def modify(self, alpha_w=0.01, alpha_b=0.01, prob_reset=0.1):\n",
    "        '''Adds perturbation to weights and biases, scaled by alphas, with random reset probability.'''\n",
    "\n",
    "        if rand() < prob_reset:\n",
    "            self.reset()\n",
    "\n",
    "        # Make a random adjustment to the weight matrix and bias vector.\n",
    "        # TODO\n",
    "        self.W = \n",
    "        self.b = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implement the SimpleHillClimber Agent\n",
    "\n",
    "In this part, we will implement a simple hill climber.  The agent has two functions, 1) update the policy given an observation and reward and 2) act on an observation (evaluate the policy).  To help get you started, the next cell has the code which we will use to place our agent in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(env, agent, num_epsiodes = 50 , max_episode_length=200, train=False):\n",
    "\n",
    "    R = 0 \n",
    "\n",
    "    # For n episodes ...\n",
    "    for i in range(num_epsiodes):\n",
    "\n",
    "        s_t = env.reset()\n",
    "\n",
    "        # For each time step ...\n",
    "        for t in range(max_episode_length):\n",
    "\n",
    "            # Draw\n",
    "            if not train:\n",
    "                env.render()\n",
    "\n",
    "            # Act\n",
    "            a_t = agent.act(s_t, training=train)\n",
    "\n",
    "            # Step\n",
    "            s_t, r_t, done, info = env.step(a_t)\n",
    "\n",
    "            # Update\n",
    "            agent.update_policy(s_t, r_t, done)\n",
    "\n",
    "            R = R + r_t\n",
    "            if done:\n",
    "                break\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "    return (1.0 / num_epsiodes) * R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outer loop in the code above loops over the number of \"episodes.\" An episode is a single simulation from beginning to end, starting from a clean environment.  The inner loop is looping over the maximum number of iterations in each episode.  Note that it is possible for the environment to decide that the episode is complete before the maximum iterations is reached.  This may happen if the cart or pole leave some bounded domain, for example.  A single iteration is controlled by 4 steps, as shown in the code:\n",
    "\n",
    "1. Render the environment (optional)\n",
    "2. Ask the agent for an action given the current state of the system (observation)\n",
    "3. Simulate a single step in the environment (perform the action)\n",
    "4. Allow the agent to update its policy\n",
    "\n",
    "At the end of all the episodes, the function returns the average reward per episode.  Note that, for the cart and pole environment, rewards are either +1 when the pole does not fall below 15 degrees from vertical in a single step and zero otherwise.  Therefore, if we take 200 steps per episode, the best average reward we can obtain is 200.\n",
    "\n",
    "**Exercise 2.1**: Implement the SimpleHillClimber class based on Algorithm 1.  The agent will keep track of two policies, 1) the best policy so far, and 2) a random perturbation from the best policy.  \n",
    "\n",
    "At each call to update_policy, the agent will update the current return (sum of rewards) and total number of calls.  \n",
    "If the end of an episode is reached, the current value of the return should be added to a \"memory\" vector, and return and number of iterations should be set back to zero.   \n",
    "Once the number of episodes per test is reached, the agent should check if the average return per episode is improved over the current policy.  If it is, then the current policy is replaced by the trial policy.  Either way, a new trail policy should then be created using the modify function of the SLP policy and the memory vector should be reset to empty.\n",
    "\n",
    "Whenever act is called, the current trial policy should be evaluated and returned if training, otherwise the current policy should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleHillClimber():\n",
    "    '''Simple Hill Climbing Agent'''\n",
    "\n",
    "    def __init__(self, obs_space, act_space, max_episode_length=50, num_episodes_per_test=100, alpha=0.1):\n",
    "        # TODO\n",
    "\n",
    "    def update_policy(self,obs,reward,done=False):\n",
    "        # TODO\n",
    "            \n",
    "\n",
    "    def act(self,obs,reward=None,done=False,training=False):\n",
    "        # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.2**: Run the training loop below.  You should see the average reward increasing as the number of episodes increases.  Note, that since we are using a rather simple algorithm here, you may need to restart this a few times to get a good result.  If you have tried it 3-5 times with no improvement, then you may have a bug in your hill climber agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = wrap_env(gym.make('CartPole-v1'))\n",
    "env.reset()\n",
    "agent = SimpleHillClimber(\n",
    "    env.observation_space, env.action_space, \n",
    "    max_episode_length=200, num_episodes_per_test=100, alpha=0.1)\n",
    "\n",
    "t = 0\n",
    "average_returns = [0,0,0,0,0,0,0,0,0,0,0]\n",
    "while np.min(average_returns[-10:-1]) < 150:\n",
    "\n",
    "    # Run the agent in the environment\n",
    "    R = run_agent(env, agent, train=True)\n",
    "\n",
    "    # Monitoring\n",
    "    average_returns.append(R)\n",
    "    t += 1\n",
    "\n",
    "    print(\"Episode {0}, average return: {1}\".format(t*50, R))\n",
    "\n",
    "plt.plot(range(len(average_returns)),average_returns)\n",
    "plt.title(\"Average reward on 100 episodes\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.3**: Run the agent for a single episode to see an animation of your trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finished, show animation; training curve\n",
    "env = wrap_env(gym.make('CartPole-v1'))\n",
    "env.reset()\n",
    "R = run_agent(env, agent, num_epsiodes=1, train=False)\n",
    "show_video()\n",
    "print(R)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going further\n",
    "\n",
    "There are a number of improvements that you can make to the simple hill climber we made here.  For example, you can use simulated annealing or beam search which you saw in the last lecture.  If you have time, consider adding one or more of these improvements.  Finally, the agent and code we created above is completely general.  Therefore you can try running on other environments.  For a complete list, look [here](https://gym.openai.com/docs/#available-environments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
