{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rhy9pCfqnw0t"
   },
   "source": [
    "# CSE 204 Lab 9: Decision Trees and Ensemble Methods\n",
    "\n",
    "J.B. Scoggins\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jbscoggi/teaching/blob/master/Polytechnique/CSE204/Lab9_answers.ipynb) \n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jbscoggi/teaching/master?filepath=Polytechnique%2FCSE204%2FLab9_answers.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this lab you will learn how to use decision trees and ensemble methods to build a model that predicts the average price of houses for neighborhoods in the US state of California.  We will make heavy use of the following libraries to play with the dataset and build our models:\n",
    "\n",
    "- [Pandas](https://pandas.pydata.org/) - python data analysis library\n",
    "- [Scikit-learn](https://scikit-learn.org/stable/) - python machine learning library\n",
    "\n",
    "Recall from the lecture that decision trees can be a powerful way to generate cheap and efficient models for classification and regression.  Some of the advantages of decision trees over other models include:\n",
    "\n",
    "- They are easy to use, requiring little data preprocessing,\n",
    "- Can be easily interpreted,\n",
    "- Are useful for feature selection,\n",
    "- Fast to build and evaluate,\n",
    "- Non-linear.\n",
    "\n",
    "On the other hand, some of the disadvantages include:\n",
    "\n",
    "- Greedy tree building algorithms are not necessarily optimal (this is NP-complete),\n",
    "- The number of samples is logarithmic in tree depth,\n",
    "- Trees are unstable, meaning they can be easily perturbed with small differences in data,\n",
    "- Decision trees are excellent overfitters,\n",
    "- Since they only consider a single feature at a time, they have difficulty handling model additivity.\n",
    "\n",
    "Often, it decision trees can be poor classifiers or regressors.  However, because of their fast training speed, they can be used to generate ensemble models, such as random forests, with boosting and bagging.  We will play with some of these concepts during this lab in order to compare to basic decision tree performance.  Before we get started, let's import and load the different packages we will use.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "colab_type": "code",
    "id": "V16HlR0yGw_0",
    "outputId": "d32c6c31-2fa6-4c09-cb7b-cb952be85a2a"
   },
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor\n",
    "\n",
    "import graphviz\n",
    "\n",
    "# Setup pandas options\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.1f}'.format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Pandas\n",
    "\n",
    "As we will be using the Pandas library, it is important you remember the core functionality from your previous labs.\n",
    "If you want a quick Pandas refresher, you can follow a simple Pandas tutorial provided by the [Google Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/).  If you are not familiar with this course, you may find it useful to supplement your lectures and labs during your free time.  [Spend the next few minutes going through the tutorial, if needed.](https://colab.research.google.com/notebooks/mlcc/intro_to_pandas.ipynb?utm_source=mlcc&utm_campaign=colab-external&utm_medium=referral&utm_content=pandas-colab&hl=en#scrollTo=TJffr5_Jwqvd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Get to know your dataset\n",
    "\n",
    "In this lab we will make use of the [California housing price dataset](https://developers.google.com/machine-learning/crash-course/california-housing-data-description).  This data was compiled from the 1990 census taken in California.  You can download the dataset with the Pandas code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the housing data\n",
    "housing_data = pd.read_csv(\n",
    "    \"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\", sep=\",\")\n",
    "\n",
    "# # Reindex and scale median house value\n",
    "# housing_data = housing_data.reindex(\n",
    "#     np.random.permutation(housing_data.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you are probably well aware at this point, one of the most important aspects of machine learning and data science is to first understand your dataset.  Whenever you are introduced to a new dataset, it is crucial that learn everything you can about the data to help inform your model building strategy.  The Pandas library is very useful for this purpose.\n",
    "\n",
    "**Exercise 1.1:** Using the describe() method to print information about the `housing_data` data frame and answer the following questions:\n",
    "\n",
    "1. How many features are in the dataset?\n",
    "2. How many samples are there?\n",
    "3. Do mean and standard deviations make sense for what you expect each variable to be?\n",
    "  - Are the latitude and longitude values consistent with California? (don't be afraid to check with Google maps)\n",
    "  - What do you think the units of `total_rooms` and `total_bedrooms` are?\n",
    "  - What about the `median_income`?\n",
    "4. Which feature are we likely to want to try and predict?\n",
    "5. Can you detect if there are any outliers in the data based on the minimum and maximum values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data.describe()\n",
    "\n",
    "# 9 features\n",
    "# 17000 samples\n",
    "# yes, rooms per block group, maybe in $10,000\n",
    "# median_house_value\n",
    "# The minimum values for total_(bed)rooms, population, and households seem very low and the max\n",
    "# median_house_value is suspicious"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.2** From the previous question, it seems logical to think that the `total_rooms` and `total_bedrooms` are for an entire block, not a single house.  Let's check this assumption by creating two new variables called `rooms_per_person` and `bedrooms_per_person`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data[\"rooms_per_person\"] = housing_data[\"total_rooms\"] / housing_data[\"population\"]\n",
    "housing_data[\"bedrooms_per_person\"] = housing_data[\"total_bedrooms\"] / housing_data[\"population\"]\n",
    "housing_data.describe()\n",
    "\n",
    "# The maximum rooms still seems strange, like corrupted data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.3:** Get a visual feel for the data by plotting the median house value on a scatter plot versus longitude and latitude (use color for house value).  Compare with a map of california such as [this one](https://www.google.com/search?q=california+maps&client=firefox-b-d&tbm=isch&source=iu&ictx=1&fir=KnETshNcnsi1VM%253A%252CMK2MjhZw7xRERM%252C_&vet=1&usg=AI4_-kSz1S_ut8rli9wcyp0A12LG1aVofg&sa=X&ved=2ahUKEwju59vrxsDhAhXqyIUKHZAkBBgQ9QEwBXoECAgQDg#imgrc=KnETshNcnsi1VM:).  (Hint: normalize the median house value by the maximum.)\n",
    "\n",
    "- Does this make sense?\n",
    "- Where are the most expensive homes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    housing_data['longitude'], housing_data['latitude'], \n",
    "    cmap=\"coolwarm\",\n",
    "    c=housing_data['median_house_value'] / housing_data['median_house_value'].max()\n",
    ")\n",
    "\n",
    "# Yes, looks like California\n",
    "# San Francisco and Los Angeles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.4:** Before building a model, let's build some expectation for what the important features are.  This will help us interpret our decision trees later on.  Use the `corr()` method from Pandas to build a correlation matrix.  Recall that a correlation matrix tells us how correlated any two features are.  For very positive values (close to 1.0), there is a strong correlation betwee the two features.  Likewise, for very negative values (close to -1.0), it means that the parameters are negatively correlated.  When the values are close to 0, there is no correlation.\n",
    "\n",
    "If you want a visual representation of the correlation matrix, you can plot it using the `imshow()` method from matplotlib.\n",
    "\n",
    "List which features (or groups of features) are strongly correlated (positive or negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data.corr()\n",
    "\n",
    "# lat/lon\n",
    "# total_(bed)rooms/households\n",
    "# median_house_value/median_income\n",
    "# (bed)rooms_per_person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.5:** Use scatter plots to visualize the correlations between the groups you identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(housing_data['longitude'], housing_data['latitude'])\n",
    "\n",
    "# plt.scatter(housing_data['total_rooms'], housing_data['total_bedrooms'])\n",
    "# plt.scatter(housing_data['total_rooms'], housing_data['population'])\n",
    "# plt.scatter(housing_data['total_rooms'], housing_data['households'])\n",
    "\n",
    "plt.scatter(housing_data['median_income'], housing_data['median_house_value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.6:** Notice anything strange about the scatter plot with `median_house_value`?  What's going here?  Use a histogram to get an idea of the distribution of this feature. \n",
    "\n",
    "- What does the histogram tell us about the distribution?\n",
    "- What could this mean about how the data was collected or processed to build the database?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "qzY2OlqgnkKQ"
   },
   "outputs": [],
   "source": [
    "housing_data['median_house_value'].hist(bins=20)\n",
    "\n",
    "# There is a peak at 500000\n",
    "# Housing values were clipped at $500,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train a simple decision tree to predict median house values\n",
    "\n",
    "At this point we have pretty good idea about our dataset.  Let's try and create a simple model based on a decision tree.\n",
    "\n",
    "**Exercise 2.1:** Split the dataset into a training and validation set.  Use the [`train_test_split()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function from Scikit-learn with `shuffle` set to `False`.  Check that the reduced datasets make sense with the `describe()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'median_house_value'\n",
    "X = housing_data.drop(target, axis=1)\n",
    "y = housing_data[target]\n",
    "\n",
    "training_features, testing_features, training_target, testing_target = train_test_split(\n",
    "    X, y, test_size=0.3, shuffle=True)\n",
    "training_features.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.2:** As a second check, redo the scatter plot from Exercise 1.3 for both the training and testing subsets.  \n",
    "\n",
    "- Is this what you expected?  \n",
    "- What went wrong?\n",
    "\n",
    "This is an important lesson.  In all of our dataset checking from Exercise 1, we didn't get a feel for the ordering of the data.  Ideally, we want our validation and training data to be identically distributed, however we did not take into account that the dataset has a clear ordering.  Normally, the `train_test_split()` defaults to shuffle our data for us to avoid such issues but we have turned this off.  Go ahead and set `shuffle` to `True` above and rerun the cell and your scatter plots below to verify this fixes the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2)\n",
    "axes[0].scatter(\n",
    "    training_features['longitude'], training_features['latitude'], \n",
    "    cmap=\"coolwarm\",\n",
    "    c=training_target / training_target.max()\n",
    ")\n",
    "axes[1].scatter(\n",
    "    testing_features['longitude'], testing_features['latitude'], \n",
    "    cmap=\"coolwarm\",\n",
    "    c=testing_target / testing_target.max()\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.3:** Use the `DecisionTreeRegressor` from Scikit-learn to create a new decision tree model with a depth of 3 and train it on the training dataset using the `fit()` method.  You can find the API for DecisionTreeRegressor [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeRegressor(max_depth=10)\n",
    "model.fit(training_features, training_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.4:** Let's get a sense of how well the tree fits the data.  Use the tree's `predict` method on the testing features to get a prediction for the testing dataset.  Compute the mean squared error using the `mean_squared_error()` function from Scikit-learn.  Also, plot a scatter plot of the predicted median house values versus the testing target values. \n",
    "\n",
    "- Is the prediction good?\n",
    "- What does the scatter plot look like?\n",
    "- How many unique values are there in our predictions? Why is this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(testing_features)\n",
    "print(mean_squared_error(prediction, testing_target))\n",
    "plt.scatter(testing_target, prediction)\n",
    "\n",
    "# Nope\n",
    "# Several constant regions\n",
    "# 8, this is 2^3, since we split the data into two groups at each split and we have 3 layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.5** You can visualize the trained decision tree with the following code.  As stated in the beginning, one advantage of such trees is that they are easily interpretible with graphs like this one.  \n",
    "\n",
    "- What are the main features used in the tree?\n",
    "- How do these compare to features you identified in Exercise 1.4?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = export_graphviz(\n",
    "    model, out_file=None, feature_names=list(training_features), filled=True, \n",
    "    rounded=True, special_characters=True)\n",
    "graph = graphviz.Source(dot_data) \n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.6** Repeat 2.3 and 2.4 for different values of the maximum tree-depth.\n",
    "\n",
    "- How does the fit improve with increasing depth?\n",
    "\n",
    "Note that you can try visualizing the tree with larger depth values, but it becomes difficult as the tree grows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Ensemble methods\n",
    "\n",
    "We have seen that a single decision tree is not a very good estimator.  We can improve our predictions using a number of ensemble techniques such as bagging, random forests, and boosting.  Using Scikit-learn, it is easy to try all of these models.  Here are the main classes that we will test:\n",
    "\n",
    "- [BaggingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html#sklearn.ensemble.BaggingRegressor)\n",
    "- [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n",
    "- [ExtraTreesRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html)\n",
    "- [AdaBoostRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html)\n",
    "\n",
    "Follow each link to read about the details of each method.  You may also want to revisit the lecture notes on ensemble methods for a quick refresher of these terms.  In the following exercises you will train each type of regressor using a max_depth of 10 layers and test their prediction accuracy.  Finally, you can perform a hyperparameter study to try and find the optimal model/parameter pair which minimizes the MSE on the testing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.1:** Create and fit a bagging regressor based on decision trees with a max depth of 10 using 100 trees, and maximum samples and features of 50%.  Compute the MSE on the testing dataset and compare the true median house values with the predictions using a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging = BaggingRegressor(\n",
    "    DecisionTreeRegressor(max_depth=10), \n",
    "    n_estimators=100, max_samples=0.5, max_features=0.5\n",
    ")\n",
    "bagging.fit(training_features, training_target)\n",
    "bagging_prediction = bagging.predict(testing_features)\n",
    "print(mean_squared_error(bagging_prediction, testing_target))\n",
    "plt.scatter(testing_target, bagging_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.2:** Create and fit a random forest regressor with a max depth of 10.  Compute the MSE on the testing dataset and compare the true median house values with the predictions using a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestRegressor(max_depth=10)\n",
    "random_forest.fit(training_features, training_target)\n",
    "rf_prediction = random_forest.predict(testing_features)\n",
    "print(mean_squared_error(rf_prediction, testing_target))\n",
    "plt.scatter(testing_target, rf_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.3:** Create and fit an extremely randomized trees regressor (ExtraTreesRegressor) with a max depth of 10.  Compute the MSE on the testing dataset and compare the true median house values with the predictions using a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomized_trees = ExtraTreesRegressor(max_depth=10)\n",
    "randomized_trees.fit(training_features, training_target)\n",
    "rt_prediction = randomized_trees.predict(testing_features)\n",
    "print(mean_squared_error(rt_prediction, testing_target))\n",
    "plt.scatter(testing_target, rt_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.4:** Create and fit a tree ensemble regressor with boosting (AdaBoostRegressor) with a max depth of 10.  Compute the MSE on the testing dataset and compare the true median house values with the predictions using a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boosted = AdaBoostRegressor(\n",
    "    DecisionTreeRegressor(max_depth=10), n_estimators=10)\n",
    "boosted.fit(training_features, training_target)\n",
    "boosted_prediction = boosted.predict(testing_features)\n",
    "print(mean_squared_error(boosted_prediction, testing_target))\n",
    "plt.scatter(testing_target, boosted_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.5:** Plot the MSE for each of the four ensemble methods above versus the maximum depth used (from 2 to 20).\n",
    "\n",
    "- At what maximum depth do the different methods more or less converge?\n",
    "- Which is the best method in terms of minimum MSE obtained?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_error(model):\n",
    "    model.fit(training_features, training_target)\n",
    "    prediction = model.predict(testing_features)\n",
    "    return mean_squared_error(prediction, testing_target)\n",
    "\n",
    "\n",
    "min_depth = 2\n",
    "max_depth = 20\n",
    "\n",
    "estimators = 100\n",
    "\n",
    "mse = np.zeros((max_depth-min_depth+1, 4))\n",
    "for i,D in enumerate(range(2,max_depth+1)):\n",
    "    print(D)\n",
    "    model = BaggingRegressor(\n",
    "        DecisionTreeRegressor(max_depth=D), \n",
    "        n_estimators=estimators, max_samples=0.5, max_features=0.5\n",
    "    )\n",
    "    mse[i,0] = test_error(model)\n",
    "    \n",
    "    model = RandomForestRegressor(max_depth=D, n_estimators=estimators)\n",
    "    mse[i,1] = test_error(model)\n",
    "    \n",
    "    model = ExtraTreesRegressor(max_depth=D, n_estimators=estimators)\n",
    "    mse[i,2] = test_error(model)\n",
    "    \n",
    "    model = AdaBoostRegressor(\n",
    "        DecisionTreeRegressor(max_depth=D), n_estimators=estimators)\n",
    "    mse[i,3] = test_error(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(min_depth, max_depth+1), mse[:,0])\n",
    "plt.plot(range(min_depth, max_depth+1), mse[:,1])\n",
    "plt.plot(range(min_depth, max_depth+1), mse[:,2])\n",
    "plt.plot(range(min_depth, max_depth+1), mse[:,3])\n",
    "plt.legend(['Bagging', 'Random Forest', 'Extremely Random Tree', 'Boosted'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "That's it for this lab!  Note there are still several ways you can try and improve the regression.  It might even be worth trying to use an entirely different model such as a neural network to see if you can obtain better performance.  Over time, you will build up an intuition for which models might work best on different datasets.  \n",
    "\n",
    "If you want more practice with trees and ensemble methods, checkout the Iris dataset below which is a classification problem.  You can use everything you learned here by replacing \"Regressor\" with \"Classifier\".  Happy coding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
