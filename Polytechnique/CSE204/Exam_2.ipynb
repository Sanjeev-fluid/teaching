{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 204 Exam 2\n",
    "\n",
    "J.B. Scoggins\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jbscoggi/teaching/blob/master/Polytechnique/CSE204/Exam_2.ipynb) \n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jbscoggi/teaching/master?filepath=Polytechnique%2FCSE204%2FExam_2.ipynb)\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this exam, you will need to apply what you have learned during previous lab exercises to build a 2-dimensional embedding for a multi-label classification dataset you have never seen before.  The dataset is already loaded and split into `labels` and `features` dataframes for you.  Be sure to carefully inspect the labels and features for yourself before continuing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset and split into labels and features dataframes\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/jbscoggi/teaching/master/Polytechnique/CSE204/data/Scene_6.csv')\n",
    "labels = data.loc[:,'beach':'urban']\n",
    "features = data.loc[:,'Att1':]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the dataset has 6 labels associated with a 294 features.  Labels are given values of 0 or 1, indicating if the example belongs to that label (1) or not (0).  Note that examples may belong to multiple labels.  In other words, the labels are not mutually exclusive.  This is known as a multi-label classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a unique floating point value for every combination of labels\n",
    "Since examples can belong to 1 or more of 6 possible labels, it may be useful to assign a unique floating point value to each possible combination of labels, in order to provide a color scale when plotting the 2D embeddings.  You can use the following formula,\n",
    "$$\n",
    "\\text{color_scale} = \\log_2 (\\sum_i y_i 2^i),\n",
    "$$\n",
    "where $y_i$ is the true i-th label for the given example.  A small code snippet is provided below, which implements this formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a floating point scale for different possibility of labels\n",
    "pow2 = np.array([np.power(2,i) for i in range(len(labels.columns))])\n",
    "colors = [np.log2(pow2.dot(row)) for index, row in labels.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting embeddings\n",
    "\n",
    "As you saw during the autoencoders lab, we can plot our dataset in a lower dimensional space.  Recall that autoencoders are a dimension-reduction technique, where the aim is to produce a low-dimenional encoding of the high-dimensional feature-space, such that we can reconstruct that space to a desired degree of accuracy.  Embeddings are similar, in that they are low-dimenional encodings, but optimized to reconstruct the labels, rather than the features.  In this exam, you are asked to produce a 2D embedding of the given dataset and plot the embedding (low dimensional transformation of the features), as you did during the autoencoders lab.  As an example of such a plot, the code below plots the first two components of the PCA analysis on the feature space. (Note the use of our `colors` list from above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_transform = PCA(2).fit_transform(features)\n",
    "plt.scatter(pca_transform[:,0], pca_transform[:,1], c=colors, cmap='rainbow')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "\n",
    "Your task is to build and train a model that produces a 2D embedding of the given dataset and then using this embedding, predicts the multi-label classification.  Note, this is similar to the autoencoders you have already built, but instead of trying to output the input, you want to output the class probabilities.  For example, if you were to use a simple linear autoencoder-like structure, it could have an input layer of 294 nodes, a code layer of 2 nodes, and an output of 6 nodes with sigmoid activations on the output layer.  However, you are not restricted to using this model.  \n",
    "\n",
    "### Grading \n",
    "\n",
    "You will be graded based on a small report that you must submit on the Moodle as a PDF.  The report should contain \n",
    "\n",
    "1. The code and a description of your model, data preprocessing, and training procedure.  The model description should include the number of total training parameters in your model.\n",
    "2. A plot of your 2D embedding, like the scatter plot shown above.\n",
    "3. The best achieved accuracy in terms of classification of each label.  We will consider the least accurate label during the grading.\n",
    "\n",
    "The first and second points above will be given a maximum of 5 points each.  Another 5 points will be determined based on how well your accuracy and number of model parameters compares with that of your classmates.  You will recieve points on a linear scale, such that the student with the best accuracy obtained will get 3 points, while the worst accuracy will get 0.  Likewise, the student with the lowest number of parameters will get 2 points, and the highest will recieve 0.  Thus there are a total of 15 points possible.\n",
    "\n",
    "## Bonus Point\n",
    "\n",
    "Bonus point: The following code loads a dataset into a 2D numpy array `X`. Build a 2D representation for this data (e.g., by modifying one of your previous architectures) which is interesting to visualize. Plotting code is provided, as well as for labelings for a set of randomly-selected rows. \n",
    "Evaluation will be done by an expert in the domain upon visual inspection of your plot. Note that there is no guarantee that it is possible to obtain a very good visual representation -- hence why it is a bonus point.\n",
    "\n",
    "```python\n",
    "X = numpy.loadtxt('https://raw.githubusercontent.com/jbscoggi/teaching/master/Polytechnique/CSE204/data/DATASET_X.csv')\n",
    "\n",
    "# TODO: create a 2D representation/encoding Z with the same number of rows as X.\n",
    "\n",
    "Z = \n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(Z[:,0], Z[:,1])\n",
    "\n",
    "rows_of_interest = [5, 15, 88, 20, 66, 21] \n",
    "for i in rows_of_interest:\n",
    "    ax.annotate(str(i), (Z[i,0], Z[i,1]), fontsize=16, color='red')\n",
    "\n",
    "plt.show()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
